{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake-Specific Workflows\n",
    "\n",
    "This notebook demonstrates advanced Snowflake-specific workflows that showcase the unique value of the `langchain-snowflake` package.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Authentication Methods** - Compare all 4 authentication approaches\n",
    "2. **Connection Troubleshooting** - Diagnose and fix connection issues\n",
    "3. **Semantic Models & Views** - Use Cortex Analyst with your data models\n",
    "4. **RAG with Cortex Search** - Retrieval-Augmented Generation with Snowflake data\n",
    "5. **Async Patterns** - High-throughput async processing\n",
    "6. **Batch Processing** - Efficient bulk operations\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed `getting_started.ipynb`\n",
    "- Snowflake account with Cortex features enabled\n",
    "- Semantic models or views in your Snowflake environment\n",
    "- Cortex Search service set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Authentication Methods Comparison\n",
    "\n",
    "The `langchain-snowflake` package supports 4 authentication methods. Let's compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_snowflake import get_default_session\n",
    "\n",
    "print(\"Authentication Methods Comparison\")\n",
    "\n",
    "# Method 1: Environment Variables (Most Flexible)\n",
    "print(\"\\n1 Environment Variables (create_session_from_env)\")\n",
    "print(\" Uses SNOWFLAKE_* environment variables\")\n",
    "print(\" Supports password, PAT, and key pair\")\n",
    "print(\" Best for development and CI/CD\")\n",
    "print(\" Easy to switch between auth methods\")\n",
    "\n",
    "# Method 2: Personal Access Token (Production Recommended)\n",
    "print(\"\\n2 Personal Access Token (create_session_from_pat)\")\n",
    "print(\" Most secure for production\")\n",
    "print(\" No password exposure\")\n",
    "print(\" Easy token rotation\")\n",
    "print(\" Requires PAT setup in Snowflake\")\n",
    "\n",
    "# Method 3: Key Pair (Enterprise)\n",
    "print(\"\\n3 Key Pair Authentication (create_session_from_key_pair)\")\n",
    "print(\" Most secure (no shared secrets)\")\n",
    "print(\" Supports private key rotation\")\n",
    "print(\" Best for enterprise environments\")\n",
    "print(\" Requires key pair generation and setup\")\n",
    "\n",
    "# Method 4: Connection String (Simple)\n",
    "print(\"\\n4 Connection String (create_session_from_connection_string)\")\n",
    "print(\" Simple one-line setup\")\n",
    "print(\" Compatible with existing workflows\")\n",
    "print(\" Exposes credentials in string\")\n",
    "print(\" Less secure for production\")\n",
    "\n",
    "print(\"\\nSmart Default (get_default_session)\")\n",
    "print(\" Tries all methods in priority order\")\n",
    "print(\" Key Pair → PAT → Password → Connection String\")\n",
    "print(\" Best for robust applications\")\n",
    "\n",
    "# Demonstrate the smart default\n",
    "try:\n",
    "    session = get_default_session()\n",
    "    if session:\n",
    "        print(\"\\nSuccessfully connected using smart default!\")\n",
    "        result = session.sql(\n",
    "            \"SELECT CURRENT_USER() as user, CURRENT_WAREHOUSE() as warehouse\"\n",
    "        ).collect()\n",
    "        print(f\" User: {result[0]['USER']}\")\n",
    "        print(f\" Warehouse: {result[0]['WAREHOUSE']}\")\n",
    "    else:\n",
    "        print(\"\\nNo valid authentication method found\")\n",
    "        print(\" Please set up at least one authentication method\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAuthentication failed: {e}\")\n",
    "    print(\" See connection troubleshooting section below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semantic Models & Views with Cortex Analyst\n",
    "\n",
    "**This is pure Snowflake gold!** Use Cortex Analyst with your semantic models and views for intelligent data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_snowflake import SnowflakeCortexAnalyst\n",
    "\n",
    "# Use the session from authentication above\n",
    "if \"session\" not in locals():\n",
    "    session = get_default_session()\n",
    "\n",
    "print(\" Cortex Analyst with Semantic Models & Views\")\n",
    "\n",
    "# Option A: Using Semantic Model File\n",
    "print(\"\\nA) Using Semantic Model File\")\n",
    "print(\" Semantic models provide rich context about your data relationships\")\n",
    "\n",
    "# Replace with your actual semantic model path\n",
    "SEMANTIC_MODEL_FILE = \"@your_stage.schema.path/your_model.yaml\"\n",
    "\n",
    "try:\n",
    "    # Create analyst with semantic model\n",
    "    analyst_with_model = SnowflakeCortexAnalyst(\n",
    "        session=session,\n",
    "        semantic_model_file=SEMANTIC_MODEL_FILE,\n",
    "        use_rest_api=True,  # Recommended for semantic models\n",
    "    )\n",
    "\n",
    "    # Example queries with rich context\n",
    "    model_queries = [\n",
    "    \"What are our top performing products by revenue this quarter?\",\n",
    "    \"Show me customer retention trends by region\",\n",
    "    \"Compare sales performance across different customer segments\"\n",
    "    ]\n",
    "\n",
    "    print(f\" Model file: {SEMANTIC_MODEL_FILE}\")\n",
    "    for query in model_queries:\n",
    "        print(f\"\\n Query: {query}\")\n",
    "        try:\n",
    "            result = analyst_with_model.run(query)\n",
    "            print(f\" SQL: {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Semantic model setup needed: {e}\")\n",
    "    print(\" Create a semantic model in your Snowflake environment first\")\n",
    "\n",
    "# Option B: Using Semantic View\n",
    "print(\"\\nB) Using Semantic View\")\n",
    "print(\" Semantic views are pre-built views with business logic\")\n",
    "\n",
    "# Replace with your actual semantic view name\n",
    "SEMANTIC_VIEW = \"your_database.your_schema.your_semantic_view\"\n",
    "\n",
    "try:\n",
    "    # Create analyst with semantic view\n",
    "    analyst_with_view = SnowflakeCortexAnalyst(\n",
    "        session=session, semantic_view=SEMANTIC_VIEW, use_rest_api=True\n",
    "    )\n",
    "\n",
    "    view_queries = [\n",
    "    \"Show me the latest KPI dashboard metrics\",\n",
    "    \"What are the current trends in our main business metrics?\",\n",
    "    \"Generate a summary report of this month's performance\"\n",
    "    ]\n",
    "\n",
    "    print(f\" View: {SEMANTIC_VIEW}\")\n",
    "    for query in view_queries:\n",
    "        print(f\"\\n Query: {query}\")\n",
    "        try:\n",
    "            result = analyst_with_view.run(query)\n",
    "            print(f\" SQL: {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Semantic view setup needed: {e}\")\n",
    "    print(\" Create a semantic view in your Snowflake environment first\")\n",
    "\n",
    "# Option C: Standard Analyst (for comparison)\n",
    "print(\"\\nC) Standard Analyst (No Semantic Context)\")\n",
    "print(\" Basic text-to-SQL without semantic understanding\")\n",
    "\n",
    "standard_analyst = SnowflakeCortexAnalyst(session=session)\n",
    "\n",
    "standard_query = \"Count total records in the customers table\"\n",
    "print(f\"\\n Query: {standard_query}\")\n",
    "try:\n",
    "    result = standard_analyst.run(standard_query)\n",
    "    print(f\" SQL: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG with Cortex Search\n",
    "\n",
    "Retrieval-Augmented Generation using Snowflake's native Cortex Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_snowflake import ChatSnowflake, SnowflakeCortexSearchRetriever\n",
    "\n",
    "print(\"RAG with Cortex Search\")\n",
    "\n",
    "# Replace with your actual Cortex Search service\n",
    "CORTEX_SEARCH_SERVICE = \"your_cortex_search_service\"\n",
    "\n",
    "try:\n",
    "    # Initialize retriever with auto-formatting for RAG\n",
    "    retriever = SnowflakeCortexSearchRetriever(\n",
    "        session=session,\n",
    "        service_name=CORTEX_SEARCH_SERVICE,\n",
    "        k=5,  # Number of documents to retrieve\n",
    "        auto_format_for_rag=True,  # Automatic document formatting\n",
    "    )\n",
    "\n",
    "    # Initialize chat model\n",
    "    llm = ChatSnowflake(\n",
    "        session=session, model=\"claude-3-5-sonnet\", temperature=0.1, max_tokens=1000\n",
    "    )\n",
    "\n",
    "    # Create RAG prompt template\n",
    "    rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Answer the question based on the following context from Snowflake Cortex Search:\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Provide a comprehensive answer based on the retrieved context. If the context doesn't contain enough information, say so clearly.\n",
    "    \"\"\")\n",
    "\n",
    "    # Build RAG chain\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()} | rag_prompt | llm\n",
    "    )\n",
    "\n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "    \"What are the main features of our product?\",\n",
    "    \"How do customers typically use our service?\",\n",
    "    \"What are the most common customer questions?\"\n",
    "    ]\n",
    "\n",
    "    print(f\"Search Service: {CORTEX_SEARCH_SERVICE}\")\n",
    "    print(\"Auto-formatting: Enabled\")\n",
    "    print(f\" LLM: {llm.model}\")\n",
    "\n",
    "    for query in test_queries:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(f\"Question: {query}\")\n",
    "\n",
    "        try:\n",
    "            # Test retrieval only\n",
    "            print(\"\\nRetrieved Documents:\")\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            print(f\" Found {len(docs)} documents\")\n",
    "            for i, doc in enumerate(docs[:2]):  # Show first 2\n",
    "                print(f\" Doc {i + 1}: {doc.page_content[:100]}...\")\n",
    "\n",
    "            # Test full RAG\n",
    "            print(\"\\n RAG Response:\")\n",
    "            response = rag_chain.invoke(query)\n",
    "            print(f\" {response.content}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error: {e}\")\n",
    "\n",
    "    print(\"\\nRAG pipeline working!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"RAG setup needed: {e}\")\n",
    "    print(\"\\nSetup Instructions:\")\n",
    "    print(\" 1. Create a Cortex Search service in Snowflake\")\n",
    "    print(\" 2. Load documents into your search service\")\n",
    "    print(\" 3. Update CORTEX_SEARCH_SERVICE variable above\")\n",
    "    print(\" 4. See Snowflake docs for Cortex Search setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Async Patterns for High Throughput\n",
    "\n",
    "Demonstrate native async patterns with `aiohttp` and `collect_nowait()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "from langchain_snowflake import ChatSnowflake, CortexSentimentTool, CortexSummarizerTool\n",
    "\n",
    "print(\"Async Patterns for High Throughput\")\n",
    "\n",
    "# Initialize async-capable components\n",
    "async_llm = ChatSnowflake(\n",
    "    session=session, model=\"claude-3-5-sonnet\", temperature=0.1, max_tokens=200\n",
    ")\n",
    "\n",
    "sentiment_tool = CortexSentimentTool(session=session)\n",
    "summarizer_tool = CortexSummarizerTool(session=session)\n",
    "\n",
    "\n",
    "async def demonstrate_async_patterns():\n",
    "    \"\"\"Demonstrate various async patterns.\"\"\"\n",
    "\n",
    "    print(\"\\nPattern 1: Concurrent Chat Completions\")\n",
    "    print(\" Multiple LLM calls in parallel using aiohttp\")\n",
    "\n",
    "    questions = [\n",
    "        \"What is Snowflake?\",\n",
    "        \"Explain cloud computing\",\n",
    "        \"What is machine learning?\",\n",
    "        \"Define data warehousing\",\n",
    "        \"What is ETL?\",\n",
    "    ]\n",
    "\n",
    "    # Sync approach (for comparison)\n",
    "    start_time = time.time()\n",
    "    sync_results = []\n",
    "    for question in questions:\n",
    "        result = async_llm.invoke(question)\n",
    "        sync_results.append(result.content[:50] + \"...\")\n",
    "        sync_time = time.time() - start_time\n",
    "\n",
    "    print(f\" Sync time: {sync_time:.2f}s\")\n",
    "\n",
    "    # Async approach\n",
    "    start_time = time.time()\n",
    "    async_tasks = [async_llm.ainvoke(question) for question in questions]\n",
    "    async_results = await asyncio.gather(*async_tasks)\n",
    "    async_time = time.time() - start_time\n",
    "\n",
    "    print(f\" Async time: {async_time:.2f}s\")\n",
    "    print(f\" Speedup: {sync_time / async_time:.1f}x faster\")\n",
    "\n",
    "    print(\"\\nPattern 2: Concurrent Tool Execution\")\n",
    "    print(\" Multiple Cortex tools in parallel\")\n",
    "\n",
    "    texts = [\n",
    "        \"This product is absolutely amazing!\",\n",
    "        \"The service was disappointing and slow.\",\n",
    "        \"Pretty good overall, could be improved.\",\n",
    "        \"Excellent customer support experience.\",\n",
    "        \"Worst purchase I've ever made.\",\n",
    "    ]\n",
    "\n",
    "    # Async tool execution\n",
    "    start_time = time.time()\n",
    "    sentiment_tasks = [sentiment_tool._arun(text) for text in texts]\n",
    "    sentiment_results = await asyncio.gather(*sentiment_tasks)\n",
    "    async_tool_time = time.time() - start_time\n",
    "\n",
    "    print(f\" Processed {len(texts)} sentiments in {async_tool_time:.2f}s\")\n",
    "    for i, (text, sentiment) in enumerate(zip(texts, sentiment_results)):\n",
    "        print(f' {i + 1}. \"{text[:30]}...\" → {sentiment}')\n",
    "\n",
    "    print(\"\\nPattern 3: Mixed Async Operations\")\n",
    "    print(\" Combining LLM calls and tool execution\")\n",
    "\n",
    "    # Parallel summarization and sentiment analysis\n",
    "    long_text = \"Snowflake is a cloud-based data platform that revolutionizes how organizations store, process, and analyze data. With its unique architecture that separates storage and compute, Snowflake provides unprecedented flexibility and scalability.\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    summary_task = summarizer_tool._arun(long_text)\n",
    "    sentiment_task = sentiment_tool._arun(long_text)\n",
    "    llm_analysis_task = async_llm.ainvoke(\n",
    "        f\"Analyze this text in 1 sentence: {long_text}\"\n",
    "    )\n",
    "\n",
    "    summary, sentiment, llm_analysis = await asyncio.gather(\n",
    "        summary_task, sentiment_task, llm_analysis_task\n",
    "    )\n",
    "\n",
    "    mixed_time = time.time() - start_time\n",
    "\n",
    "    print(f\" Mixed operations completed in {mixed_time:.2f}s\")\n",
    "    print(f\" Summary: {summary}\")\n",
    "    print(f\" Sentiment: {sentiment}\")\n",
    "    print(f\" LLM Analysis: {llm_analysis.content}\")\n",
    "\n",
    "\n",
    "# Run async demonstration\n",
    "try:\n",
    "    await demonstrate_async_patterns()\n",
    "    print(\"\\nAsync patterns working!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAsync error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing Examples\n",
    "\n",
    "Efficient bulk operations for production workloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_snowflake import (\n",
    "    CortexSentimentTool,\n",
    "    CortexSummarizerTool,\n",
    "    CortexTranslatorTool,\n",
    ")\n",
    "\n",
    "\n",
    "async def batch_sentiment_analysis(texts: List[str], batch_size: int = 10):\n",
    "    \"\"\"Process large batches of text for sentiment analysis.\"\"\"\n",
    "    sentiment_tool = CortexSentimentTool(session=session)\n",
    "\n",
    "    print(f\"\\nBatch Sentiment Analysis ({len(texts)} texts)\")\n",
    "    print(f\" Batch size: {batch_size}\")\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Process in batches to avoid overwhelming the system\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        print(\n",
    "            f\" Processing batch {i // batch_size + 1}/{(len(texts) - 1) // batch_size + 1}\"\n",
    "        )\n",
    "\n",
    "        # Async processing within each batch\n",
    "        batch_tasks = [sentiment_tool._arun(text) for text in batch]\n",
    "        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)\n",
    "\n",
    "        results.extend(batch_results)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    successful = len([r for r in results if not isinstance(r, Exception)])\n",
    "\n",
    "    print(f\" Processed {successful}/{len(texts)} texts in {total_time:.2f}s\")\n",
    "    print(f\" Rate: {successful / total_time:.1f} texts/second\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "async def batch_document_processing(documents: List[str]):\n",
    "    \"\"\"Process documents with multiple Cortex functions.\"\"\"\n",
    "    sentiment_tool = CortexSentimentTool(session=session)\n",
    "    summarizer_tool = CortexSummarizerTool(session=session)\n",
    "    translator_tool = CortexTranslatorTool(session=session)\n",
    "\n",
    "    print(f\"\\nMulti-Function Document Processing ({len(documents)} docs)\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    tasks = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        doc_tasks = {\n",
    "            \"sentiment\": sentiment_tool._arun(doc),\n",
    "            \"summary\": summarizer_tool._arun(doc),\n",
    "            \"translation\": translator_tool._arun(\n",
    "                {\"text\": doc, \"target_language\": \"es\"}\n",
    "            ),\n",
    "        }\n",
    "        tasks.append(doc_tasks)\n",
    "\n",
    "    # Process each document with multiple functions in parallel\n",
    "    all_sentiment_tasks = [task[\"sentiment\"] for task in tasks]\n",
    "    all_summary_tasks = [task[\"summary\"] for task in tasks]\n",
    "    all_translation_tasks = [task[\"translation\"] for task in tasks]\n",
    "\n",
    "    sentiments, summaries, translations = await asyncio.gather(\n",
    "        asyncio.gather(*all_sentiment_tasks, return_exceptions=True),\n",
    "        asyncio.gather(*all_summary_tasks, return_exceptions=True),\n",
    "        asyncio.gather(*all_translation_tasks, return_exceptions=True),\n",
    "    )\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_operations = len(documents) * 3  # 3 operations per document\n",
    "\n",
    "    print(f\" Completed {total_operations} operations in {total_time:.2f}s\")\n",
    "    print(f\" Rate: {total_operations / total_time:.1f} operations/second\")\n",
    "\n",
    "    # Show sample results\n",
    "    for i, (sentiment, summary, translation) in enumerate(\n",
    "        zip(sentiments[:2], summaries[:2], translations[:2])\n",
    "    ):\n",
    "        print(f\"\\n Document {i + 1}:\")\n",
    "        print(f\" Original: {documents[i][:50]}...\")\n",
    "        if not isinstance(sentiment, Exception):\n",
    "            print(f\" Sentiment: {sentiment}\")\n",
    "        if not isinstance(summary, Exception):\n",
    "            print(f\" Summary: {summary}\")\n",
    "        if not isinstance(translation, Exception):\n",
    "            print(f\" Spanish: {translation}\")\n",
    "\n",
    "    return sentiments, summaries, translations\n",
    "\n",
    "\n",
    "# Sample data for batch processing\n",
    "sample_reviews = [\n",
    "    \"The product quality is outstanding and delivery was fast.\",\n",
    "    \"Poor customer service, very disappointed with the experience.\",\n",
    "    \"Good value for money, would recommend to others.\",\n",
    "    \"The interface is confusing and hard to navigate.\",\n",
    "    \"Excellent features and great user experience overall.\",\n",
    "    \"Too expensive for what you get, not worth it.\",\n",
    "    \"Fast shipping and product matches description perfectly.\",\n",
    "    \"Customer support was helpful and resolved my issue quickly.\",\n",
    "    \"The product broke after just one week of use.\",\n",
    "    \"Amazing quality and exceeded my expectations completely.\",\n",
    "]\n",
    "\n",
    "sample_documents = [\n",
    "    \"Snowflake's cloud data platform enables organizations to mobilize their data with Snowflake's Data Cloud. Our platform provides a unified experience across multiple public clouds, allowing customers to securely share and analyze data.\",\n",
    "    \"Machine learning capabilities in Snowflake include Snowpark ML for data science workflows, automatic feature engineering, model training, and deployment. The platform supports popular ML frameworks and languages.\",\n",
    "    \"Data governance in Snowflake ensures security, privacy, and compliance through features like role-based access control, data masking, and audit trails. Organizations can maintain control while enabling data sharing.\",\n",
    "]\n",
    "\n",
    "\n",
    "async def run_batch_examples():\n",
    "    \"\"\"Run all batch processing examples.\"\"\"\n",
    "    # Example 1: Batch sentiment analysis\n",
    "    await batch_sentiment_analysis(sample_reviews, batch_size=5)\n",
    "\n",
    "    # Example 2: Multi-function document processing\n",
    "    await batch_document_processing(sample_documents)\n",
    "\n",
    "\n",
    "try:\n",
    "    await run_batch_examples()\n",
    "    print(\"\\nBatch processing examples completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nBatch processing error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-snowflake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
